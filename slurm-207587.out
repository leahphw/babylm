Mon Apr  7 21:10:56 EDT 2025
I ran on:
SLURM_NODELIST=spdr18
cd to target directory
setting up conda
no change     /usr/local/sw/anaconda/anaconda3/condabin/conda
no change     /usr/local/sw/anaconda/anaconda3/bin/conda
no change     /usr/local/sw/anaconda/anaconda3/bin/conda-env
no change     /usr/local/sw/anaconda/anaconda3/bin/activate
no change     /usr/local/sw/anaconda/anaconda3/bin/deactivate
no change     /usr/local/sw/anaconda/anaconda3/etc/profile.d/conda.sh
no change     /usr/local/sw/anaconda/anaconda3/etc/fish/conf.d/conda.fish
no change     /usr/local/sw/anaconda/anaconda3/shell/condabin/Conda.psm1
no change     /usr/local/sw/anaconda/anaconda3/shell/condabin/conda-hook.ps1
no change     /usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/xontrib/conda.xsh
no change     /usr/local/sw/anaconda/anaconda3/etc/profile.d/conda.csh
no change     /home/km3nc/.bashrc
No action taken.
checking pip installs
WARNING: No metadata found in /usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages
minicons==0.3.22
Run eval script
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
2025-04-07 21:11:01.769519: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 21:11:01.808340: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-04-07 21:11:01.808368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-04-07 21:11:01.809319: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-07 21:11:01.814475: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-07 21:11:03.029952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-04-07:21:11:52,942 WARNING  [loading.py:546] Using the latest cached version of the module from /scratch/cfinegan/hf_cache/modules/evaluate_modules/metrics/evaluate-metric--exact_match/9d3b67e0c429cd7460b2b05aab53419b48eea369b73e1d9f185a56ca90c373d4 (last modified on Tue Apr  1 16:07:14 2025) since it couldn't be found locally at evaluate-metric--exact_match, or remotely on the Hugging Face Hub.
2025-04-07:21:11:53,109 INFO     [__main__.py:263] Verbosity set to INFO
2025-04-07:21:11:59,517 INFO     [__main__.py:349] Selected Tasks: ['blimp_filtered', 'blimp_supplement']
2025-04-07:21:11:59,523 INFO     [evaluator.py:133] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2025-04-07:21:11:59,523 INFO     [evaluator.py:181] Initializing hf-mlm model, with arguments: {'pretrained': '/home/km3nc/babylm/models/Baby-Llama-58M', 'backend': 'mlm', 'trust_remote_code': True}
2025-04-07:21:11:59,671 WARNING  [other.py:335] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-07:21:11:59,671 INFO     [huggingface.py:196] Using device 'cuda:0'
2025-04-07:21:11:59,676 INFO     [huggingface.py:489] Overrode HF model backend type, and using type 'mlm'
Traceback (most recent call last):
  File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/cfinegan/shared/evaluation-pipeline-2024/lm_eval/__main__.py", line 431, in <module>
    cli_evaluate()
  File "/home/cfinegan/shared/evaluation-pipeline-2024/lm_eval/__main__.py", line 356, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/cfinegan/shared/evaluation-pipeline-2024/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/home/cfinegan/shared/evaluation-pipeline-2024/lm_eval/evaluator.py", line 184, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/cfinegan/shared/evaluation-pipeline-2024/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/cfinegan/shared/evaluation-pipeline-2024/lm_eval/models/huggingface.py", line 235, in __init__
    self._create_model(
  File "/home/cfinegan/shared/evaluation-pipeline-2024/lm_eval/models/huggingface.py", line 609, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/home/km3nc/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 574, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForMaskedLM.
Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, ModernBertConfig, MPNetConfig, MraConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XmodConfig, YosoConfig.
pretrained=/home/km3nc/babylm/models/Baby-Llama-58M,backend=mlm,trust_remote_code=True
Command exited with non-zero status 1
	Command being timed: "./eval_blimp.sh /home/km3nc/babylm/models/Baby-Llama-58M"
	User time (seconds): 13.25
	System time (seconds): 2.17
	Percent of CPU this job got: 24%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 1:02.46
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1186408
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 2
	Minor (reclaiming a frame) page faults: 565847
	Voluntary context switches: 40629
	Involuntary context switches: 490
	Swaps: 0
	File system inputs: 1480
	File system outputs: 88
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
