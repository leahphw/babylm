git clone https://github.com/leahphw/babylm.git
git switch -c babyLM_code --track origin/babyLM_code

# To sync back
git bundle create repo.bundle --all
rsync spydur:/home/jstipl/code/babylm/repo.bundle /tmp/
git remote set-url origin git@github.com:leahphw/babylm.git
git push origin --all

python train.py --config ./config/llama-16M.yaml

# We are group 1 -> devices 0 and 1
export CUDA_VISIBLE_DEVICES=0,1

# See GPU utilization 
nvidia-smi
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1863759      C   python3.9                                    8574MiB |
|    1   N/A  N/A   1863759      C   python3.9                                    6522MiB |
+-----------------------------------------------------------------------------------------+

# This is faster, it uses the 2 GPUs fully
torchrun --nproc_per_node=2 train.py