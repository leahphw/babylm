git clone https://github.com/leahphw/babylm.git
git switch -c babyLM_code --track origin/babyLM_code

# To sync back
on spider: 
git bundle create repo.bundle --all

on your machine where you have github key:
rsync spydur:/home/jstipl/code/babylm/repo.bundle /tmp/
cd /tmp/
rm -rf /tmp/repo
git clone repo.bundle 
cd repo
git remote set-url origin git@github.com:leahphw/babylm.git
git push origin --all
git log

# To sync forward (local to spydur)
rsync -r $(pwd) spydur:/home/jstipl/code/


python train.py --config ./config/llama-16M.yaml

# We are group 1 -> devices 0 and 1
export CUDA_VISIBLE_DEVICES=0,1

# See GPU utilization 
nvidia-smi
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   1863759      C   python3.9                                    8574MiB |
|    1   N/A  N/A   1863759      C   python3.9                                    6522MiB |
+-----------------------------------------------------------------------------------------+

# This is faster, it uses the 2 GPUs fully
torchrun --nproc_per_node=2 train.py

sbatch train.py.slurm

export LAST_JOB_ID=$(sbatch train.py.slurm | cut -d" " -f 4)
tail -f slurm-$LAST_JOB_ID.out


tail -f slurm-207326.out

(base) [jstipl@spydur]: squeue | grep NLP
            202426       NLP jstipl_t   jstipl  R       0:17      1 spdr18
scancel <job_id>