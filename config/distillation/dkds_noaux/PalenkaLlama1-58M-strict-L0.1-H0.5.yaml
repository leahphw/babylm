data:
  tokenizer_path: "/scratch/nlp_G1/models/tokenizer/gpt-clean-16000.json"
  train_path: "/scratch/nlp_G1/data/train_10M_clean"
  eval_path: "/scratch/nlp_G1/data/dev_clean"
  seq_length: 128
  eval_samples: 8192

student:
  type: "Llama" 
  name: "PalenkaLlama1-58M-strict-L0.1-H0.5"
  hidden_size: 512
  intermediate_size: 1024 
  n_layer: 16
  n_head: 8
  resid_pdrop: 0.0 
  attn_pdrop: 0.0
  embd_pdrop: 0.0
  output_dir: "/scratch/nlp_G1/models/student/"

teachers:
  - type: "GPT2"
    path: "/scratch/nlp_G1/models/base_line/BabyLlama1-Teacher-GPT2-705M-strict"
    layer_mappings: {0: 0, 3: 2, 6: 4, 9: 6, 12: 8, 15: 10, 18: 12, 21: 14, 23: 15}  # GPT2-705M to Student
  - type: "Llama"
    path: "/scratch/nlp_G1/models/base_line/BabyLlama1-Teacher-Llama-360M-strict"
    layer_mappings: {0: 0, 3: 1, 6: 3, 9: 5, 12: 7, 15: 9, 18: 11, 21: 13, 23: 15}  # Llama-360M to Student

training:
  lr: 2.5e-4
  batch_size: 32
  num_epochs: 6
  gradient_accumulation_steps: 1
  warmup_steps: 200
  weight_decay: 0.1
  fp16: True
  temperature: 2.0
  hard_target_loss_weight: 1.0  
  logit_distillation_loss_weight: 0.1
  hidden_distillation_loss_weight: 0.5





  
