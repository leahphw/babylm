Mon Apr  7 18:05:07 EDT 2025
I ran on:
SLURM_NODELIST=spdr18
Moving to /home/km3nc/babylm
setting up conda
no change     /usr/local/sw/anaconda/anaconda3/condabin/conda
no change     /usr/local/sw/anaconda/anaconda3/bin/conda
no change     /usr/local/sw/anaconda/anaconda3/bin/conda-env
no change     /usr/local/sw/anaconda/anaconda3/bin/activate
no change     /usr/local/sw/anaconda/anaconda3/bin/deactivate
no change     /usr/local/sw/anaconda/anaconda3/etc/profile.d/conda.sh
no change     /usr/local/sw/anaconda/anaconda3/etc/fish/conf.d/conda.fish
no change     /usr/local/sw/anaconda/anaconda3/shell/condabin/Conda.psm1
no change     /usr/local/sw/anaconda/anaconda3/shell/condabin/conda-hook.ps1
no change     /usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/xontrib/conda.xsh
no change     /usr/local/sw/anaconda/anaconda3/etc/profile.d/conda.csh
no change     /home/km3nc/.bashrc
No action taken.
checking pip installs
WARNING: No metadata found in /usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages
minicons==0.3.22
Running python script
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
W0407 18:05:11.406928 2653856 site-packages/torch/distributed/run.py:792] 
W0407 18:05:11.406928 2653856 site-packages/torch/distributed/run.py:792] *****************************************
W0407 18:05:11.406928 2653856 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0407 18:05:11.406928 2653856 site-packages/torch/distributed/run.py:792] *****************************************
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.
  warnings.warn(
2025-04-07 18:05:14.461555: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 18:05:14.461556: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 18:05:14.496057: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-04-07 18:05:14.496059: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-04-07 18:05:14.496091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-04-07 18:05:14.496092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-04-07 18:05:14.497046: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-07 18:05:14.497049: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-07 18:05:14.502206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-07 18:05:14.502206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-07 18:05:15.628594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-04-07 18:05:15.628611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loading data from /scratch/jstipl/data/train_10M_clean/tokenized_GPT2TokenizerFast_16000.pt
Loading data from /scratch/jstipl/data/train_10M_clean/tokenized_GPT2TokenizerFast_16000.pt
Loading data from /scratch/jstipl/data/dev_clean/tokenized_GPT2TokenizerFast_16000.pt
Loading data from /scratch/jstipl/data/dev_clean/tokenized_GPT2TokenizerFast_16000.pt
model num parameters: student = 58343936
model num parameters: teacher1 = 16584960
model num parameters: teacher2 = 97540608
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
model num parameters: student = 58343936
model num parameters: teacher1 = 16584960
model num parameters: teacher2 = 97540608
/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/km3nc/babylm/distill-ensemble-pretraining-baby-llama.py", line 181, in <module>
[rank1]:     trainer = DistillationTrainer(
[rank1]:   File "/home/km3nc/babylm/distill-ensemble-pretraining-baby-llama.py", line 113, in __init__
[rank1]:     super().__init__(*args, **kwargs)
[rank1]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 598, in __init__
[rank1]:     self.callback_handler = CallbackHandler(
[rank1]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py", line 409, in __init__
[rank1]:     self.add_callback(cb)
[rank1]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py", line 426, in add_callback
[rank1]:     cb = callback() if isinstance(callback, type) else callback
[rank1]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/integrations/integration_utils.py", line 767, in __init__
[rank1]:     raise RuntimeError("WandbCallback requires wandb to be installed. Run `pip install wandb`.")
[rank1]: RuntimeError: WandbCallback requires wandb to be installed. Run `pip install wandb`.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/km3nc/babylm/distill-ensemble-pretraining-baby-llama.py", line 181, in <module>
[rank0]:     trainer = DistillationTrainer(
[rank0]:   File "/home/km3nc/babylm/distill-ensemble-pretraining-baby-llama.py", line 113, in __init__
[rank0]:     super().__init__(*args, **kwargs)
[rank0]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 598, in __init__
[rank0]:     self.callback_handler = CallbackHandler(
[rank0]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py", line 409, in __init__
[rank0]:     self.add_callback(cb)
[rank0]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py", line 426, in add_callback
[rank0]:     cb = callback() if isinstance(callback, type) else callback
[rank0]:   File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/transformers/integrations/integration_utils.py", line 767, in __init__
[rank0]:     raise RuntimeError("WandbCallback requires wandb to be installed. Run `pip install wandb`.")
[rank0]: RuntimeError: WandbCallback requires wandb to be installed. Run `pip install wandb`.
[rank0]:[W407 18:05:20.918729707 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0407 18:05:20.949528 2653856 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2653865 closing signal SIGTERM
E0407 18:05:20.981458 2653856 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 2653866) of binary: /usr/local/sw/anaconda/anaconda3/bin/python
Traceback (most recent call last):
  File "/usr/local/sw/anaconda/anaconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/sw/anaconda/anaconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
distill-ensemble-pretraining-baby-llama.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-07_18:05:20
  host      : spdr18.cluster
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2653866)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Command exited with non-zero status 1
	Command being timed: "torchrun --nproc_per_node=2 distill-ensemble-pretraining-baby-llama.py"
	User time (seconds): 14.20
	System time (seconds): 4.09
	Percent of CPU this job got: 157%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:11.59
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1810536
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 862
	Minor (reclaiming a frame) page faults: 772765
	Voluntary context switches: 52726
	Involuntary context switches: 196
	Swaps: 0
	File system inputs: 16
	File system outputs: 112
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
